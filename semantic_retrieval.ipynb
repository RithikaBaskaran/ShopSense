{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ›ï¸ ShopSense â€” Phase 2: Semantic Retrieval with FAISS\n",
    "**Environment:** VS Code (local machine)\n",
    "\n",
    "### What this phase does:\n",
    "1. Loads the 10,000 clean products from Phase 1\n",
    "2. Embeds every product description using `all-MiniLM-L6-v2` (a fast, lightweight sentence transformer)\n",
    "3. Stores all embeddings in a FAISS index for fast similarity search\n",
    "4. Builds a `search()` function that takes a natural language query and returns the top 20 matching products\n",
    "\n",
    "### New files created:\n",
    "```\n",
    "shopsense/\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â”œâ”€â”€ products_clean.csv       â† from Phase 1\n",
    "â”‚   â”œâ”€â”€ faiss_index.bin          â† FAISS index (new)\n",
    "â”‚   â””â”€â”€ faiss_metadata.json      â† maps index positions â†’ product info (new)\n",
    "â”œâ”€â”€ data_preparation.ipynb\n",
    "â””â”€â”€ semantic_retrieval.ipynb     â† this file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 â€” Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sentence-transformers faiss-cpu pandas numpy tqdm -q\n",
    "print('âœ… Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 â€” Load Products from Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "\n",
    "df = pd.read_csv(DATA_DIR / 'products_clean.csv')\n",
    "\n",
    "# Drop rows with missing descriptions (safety check)\n",
    "df = df.dropna(subset=['description']).reset_index(drop=True)\n",
    "\n",
    "print(f'âœ… Loaded {len(df):,} products')\n",
    "print(f'Columns: {df.columns.tolist()}')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 â€” Prepare Text for Embedding\n",
    "We combine `title` + `description` into one string per product.\n",
    "Title is repeated at the front because it carries the most signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embed_text(row):\n",
    "    \"\"\"Combine title + description into one rich string for embedding.\"\"\"\n",
    "    title = str(row.get('title', '') or '').strip()\n",
    "    desc  = str(row.get('description', '') or '').strip()\n",
    "    # Truncate description to 300 chars to avoid token limit issues\n",
    "    desc  = desc[:300]\n",
    "    return f\"{title}. {desc}\"\n",
    "\n",
    "df['embed_text'] = df.apply(build_embed_text, axis=1)\n",
    "\n",
    "print(f'âœ… Embed text prepared for {len(df):,} products')\n",
    "print(f'Avg embed text length: {df[\"embed_text\"].str.len().mean():.0f} chars')\n",
    "print(f'\\nSample embed text:')\n",
    "print(df['embed_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 â€” Load Sentence Transformer Model\n",
    "`all-MiniLM-L6-v2` is ~80MB, downloads once and caches locally.\n",
    "It produces 384-dimensional embeddings and is fast on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "\n",
    "print(f'Loading model: {MODEL_NAME}')\n",
    "print('First run downloads ~80MB â€” cached after that...')\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "print(f'âœ… Model loaded')\n",
    "print(f'   Embedding dimension : {model.get_sentence_embedding_dimension()}')\n",
    "print(f'   Max sequence length : {model.max_seq_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 â€” Generate Embeddings for All Products\n",
    "Embeds all 10,000 product descriptions.\n",
    "Takes ~5-10 minutes on CPU â€” grab a coffee â˜•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "texts = df['embed_text'].tolist()\n",
    "\n",
    "print(f'Embedding {len(texts):,} product descriptions...')\n",
    "print('This takes ~5-10 min on CPU. Progress bar below:')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    batch_size=64,          # process 64 at a time\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True  # L2 normalize â€” makes cosine similarity = dot product\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f'\\nâœ… Embeddings generated')\n",
    "print(f'   Shape     : {embeddings.shape}  (products Ã— embedding_dim)')\n",
    "print(f'   Data type : {embeddings.dtype}')\n",
    "print(f'   Time taken: {elapsed/60:.1f} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 â€” Build FAISS Index\n",
    "FAISS (Facebook AI Similarity Search) stores all embeddings and lets us find the nearest neighbors to any query vector in milliseconds.\n",
    "\n",
    "We use `IndexFlatIP` â€” Inner Product index. Since embeddings are L2 normalized, inner product = cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "EMBEDDING_DIM = embeddings.shape[1]  # 384 for MiniLM\n",
    "\n",
    "# Build the index\n",
    "index = faiss.IndexFlatIP(EMBEDDING_DIM)   # IP = Inner Product (cosine sim)\n",
    "index.add(embeddings.astype('float32'))    # FAISS requires float32\n",
    "\n",
    "print(f'âœ… FAISS index built')\n",
    "print(f'   Index type       : IndexFlatIP (exact cosine similarity)')\n",
    "print(f'   Embedding dim    : {EMBEDDING_DIM}')\n",
    "print(f'   Vectors indexed  : {index.ntotal:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 â€” Save FAISS Index + Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save FAISS index\n",
    "index_path = DATA_DIR / 'faiss_index.bin'\n",
    "faiss.write_index(index, str(index_path))\n",
    "print(f'âœ… FAISS index saved â†’ {index_path} ({index_path.stat().st_size/1024:.0f} KB)')\n",
    "\n",
    "# Save metadata â€” maps each FAISS position (0, 1, 2...) to product info\n",
    "# This is how we go from \"index position 42\" back to the actual product\n",
    "metadata = df[['asin', 'title', 'price', 'rating', 'rating_count', 'category', 'description']].to_dict(orient='records')\n",
    "\n",
    "meta_path = DATA_DIR / 'faiss_metadata.json'\n",
    "with open(meta_path, 'w') as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "print(f'âœ… Metadata saved    â†’ {meta_path} ({meta_path.stat().st_size/1024:.0f} KB)')\n",
    "print(f'\\n   Metadata contains {len(metadata):,} product records')\n",
    "print(f'   Fields: {list(metadata[0].keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 â€” Build the Search Function\n",
    "This is the core of Phase 2. Takes a natural language query, embeds it, and returns the top-K most similar products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, top_k: int = 20) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Search for products using natural language.\n",
    "\n",
    "    Args:\n",
    "        query  : Natural language search string e.g. 'compact coffee maker for small kitchen'\n",
    "        top_k  : Number of results to return (default 20, reranker will pick top 5)\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with product info + similarity score\n",
    "    \"\"\"\n",
    "    # 1. Embed the query using the same model\n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True\n",
    "    ).astype('float32')\n",
    "\n",
    "    # 2. Search the FAISS index\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # 3. Build results list\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        if idx == -1:   # FAISS returns -1 for empty slots\n",
    "            continue\n",
    "        product = metadata[idx].copy()\n",
    "        product['similarity_score'] = round(float(score), 4)\n",
    "        results.append(product)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print('âœ… search() function ready.')\n",
    "print('Usage: results = search(\"your query here\", top_k=20)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 â€” Test the Search Function\n",
    "Let's run a few queries and see what comes back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(query: str, top_k: int = 5):\n",
    "    \"\"\"Pretty print search results.\"\"\"\n",
    "    print(f'ðŸ” Query: \"{query}\"')\n",
    "    print('â”€' * 65)\n",
    "    results = search(query, top_k=top_k)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        price_str = f\"${r['price']:.2f}\" if r['price'] else 'N/A'\n",
    "        print(f\"{i}. {r['title'][:70]}\")\n",
    "        print(f\"   Price: {price_str}  |  Rating: {r['rating']}â­ ({r['rating_count']:,} reviews)\")\n",
    "        print(f\"   Similarity: {r['similarity_score']}\")\n",
    "        print()\n",
    "    print('=' * 65)\n",
    "\n",
    "# â”€â”€ Test Query 1 â”€â”€\n",
    "display_results('compact coffee maker for small kitchen', top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Test Query 2 â”€â”€\n",
    "display_results('non stick frying pan under $30', top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Test Query 3 â”€â”€\n",
    "display_results('gift for someone who loves cooking', top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Test Query 4 â€” try your own! â”€â”€\n",
    "display_results('storage solution for small apartment', top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 â€” Reload Test (verify saved index works)\n",
    "Simulates loading the index fresh â€” exactly what Phase 3 will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing reload from disk...')\n",
    "\n",
    "# Load index from disk\n",
    "index_loaded = faiss.read_index(str(DATA_DIR / 'faiss_index.bin'))\n",
    "\n",
    "# Load metadata from disk\n",
    "with open(DATA_DIR / 'faiss_metadata.json') as f:\n",
    "    metadata_loaded = json.load(f)\n",
    "\n",
    "print(f'âœ… Index reloaded â€” {index_loaded.ntotal:,} vectors')\n",
    "print(f'âœ… Metadata reloaded â€” {len(metadata_loaded):,} records')\n",
    "\n",
    "# Quick sanity search\n",
    "q_emb = model.encode(['kitchen knife set'], normalize_embeddings=True, convert_to_numpy=True).astype('float32')\n",
    "scores, indices = index_loaded.search(q_emb, 3)\n",
    "print(f'\\nSanity search â†’ \"kitchen knife set\":')\n",
    "for score, idx in zip(scores[0], indices[0]):\n",
    "    print(f'  [{score:.4f}] {metadata_loaded[idx][\"title\"][:70]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 â€” Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 55)\n",
    "print('     PHASE 2 COMPLETE â€” SUMMARY')\n",
    "print('=' * 55)\n",
    "print(f'  Model              : {MODEL_NAME}')\n",
    "print(f'  Embedding dim      : {EMBEDDING_DIM}')\n",
    "print(f'  Products indexed   : {index.ntotal:,}')\n",
    "print(f'  Index type         : IndexFlatIP (cosine similarity)')\n",
    "print()\n",
    "print('  Files created:')\n",
    "for fname in ['faiss_index.bin', 'faiss_metadata.json']:\n",
    "    fpath = DATA_DIR / fname\n",
    "    print(f'    âœ… data/{fname} ({fpath.stat().st_size/1024:.0f} KB)')\n",
    "print()\n",
    "print('  âž¡ï¸  Next: Phase 3 â€” Cross-Encoder Reranking (Colab, GPU)')\n",
    "print('=' * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to GitHub\n",
    "```bash\n",
    "git add semantic_retrieval.ipynb\n",
    "git commit -m \"phase 2: semantic retrieval with FAISS complete\"\n",
    "git push\n",
    "```\n",
    "> `data/` is still in `.gitignore` so the large `.bin` and `.json` files won't be pushed."
   ]
  }
 ]
}
