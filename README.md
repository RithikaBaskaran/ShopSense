# üõçÔ∏è ShopSense

> AI-powered product recommendation system with semantic search, session personalization, RAG review summaries, and a LangGraph agent ‚Äî deployed on Hugging Face Spaces.

[![Live Demo](https://img.shields.io/badge/ü§ó%20Live%20Demo-ShopSense-green)](https://huggingface.co/spaces/RithikaBaskaran/shopsense)
[![Python](https://img.shields.io/badge/Python-3.10%2B-blue)](https://python.org)
[![Gradio](https://img.shields.io/badge/Gradio-5.x-orange)](https://gradio.app)
[![License](https://img.shields.io/badge/License-MIT-lightgrey)](LICENSE)

---

## üîó Live Demo

**[https://huggingface.co/spaces/RithikaBaskaran/shopsense](https://huggingface.co/spaces/RithikaBaskaran/shopsense)**

Type a natural language query like *"compact coffee maker for small kitchen"* or *"non stick frying pan under $30"* and ShopSense will retrieve, rerank, summarize, and explain the best matching products ‚Äî getting smarter with every search.

---

## üì∏ Overview

ShopSense is a full end-to-end ML portfolio project built across 9 phases, covering everything from raw data preparation to a publicly deployed web application.

| Feature | Description |
|---|---|
| üîç Semantic Search | FAISS vector search with `all-MiniLM-L6-v2` embeddings |
| üìä Reranking | Cross-encoder reranking (`ms-marco-MiniLM-L-6-v2`) for precision ordering |
| üìù Review Summaries | RAG pipeline ‚Äî retrieve relevant review chunks ‚Üí Llama 3 summarization |
| ü§ñ LangGraph Agent | Intent extraction, filter parsing, clarification, explanation generation |
| üß† Session Memory | Tracks likes, dismissals, price preferences across the conversation |
| üìà Evaluation | NDCG@5 and MRR metrics ‚Äî reranking improves NDCG by **+9.7%** |
| üîß Fine-tuning | LoRA fine-tuning of `flan-t5-base` on synthetic training data |
| üöÄ Deployment | FastAPI backend + Gradio frontend on Hugging Face Spaces |

---

## üèóÔ∏è Architecture

```
User Query
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         LangGraph Agent             ‚îÇ
‚îÇ  ‚Ä¢ Analyze intent                   ‚îÇ
‚îÇ  ‚Ä¢ Extract filters (price, rating)  ‚îÇ
‚îÇ  ‚Ä¢ Detect vague queries ‚Üí clarify   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         FAISS Search                ‚îÇ
‚îÇ  all-MiniLM-L6-v2 embeddings        ‚îÇ
‚îÇ  10,000 products ¬∑ top 20 results   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Session Personalization        ‚îÇ
‚îÇ  Re-score based on:                 ‚îÇ
‚îÇ  +0.15 keyword overlap with likes   ‚îÇ
‚îÇ  +0.10 price matches liked range    ‚îÇ
‚îÇ  -0.10 already seen this session    ‚îÇ
‚îÇ  -0.20 explicitly dismissed         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       RAG Review Summaries          ‚îÇ
‚îÇ  Review FAISS index ‚Üí top chunks    ‚îÇ
‚îÇ  ‚Üí Llama 3 ‚Üí pros/cons/verdict      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Explanation Generation         ‚îÇ
‚îÇ  Groq Llama 3 ‚Üí friendly summary    ‚îÇ
‚îÇ  referencing session history        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
            Results UI
```

---

## üìä Evaluation Results

Evaluated on 8 queries using automated relevance judgements (0‚Äì3 scale).

| Pipeline | MRR | NDCG@5 |
|---|---|---|
| FAISS Only | 1.000 | 0.910 |
| FAISS + Cross-Encoder Reranking | 1.000 | 0.998 |
| **Improvement** | **+0.0%** | **+9.7%** |

**MRR = 1.0 for both** ‚Äî FAISS already surfaces a relevant result at position 1 every time.
**NDCG improved +9.7%** ‚Äî reranking fixes the ordering within the top 5, pushing higher relevance results to the top.

---

## üìÅ Project Structure

```
ShopSense/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ products_clean.csv          # 10,000 Amazon Home & Kitchen products
‚îÇ   ‚îú‚îÄ‚îÄ reviews_clean.csv           # 5,566 review chunks (2,003 products)
‚îÇ   ‚îú‚îÄ‚îÄ faiss_index.bin             # Product embeddings (384-dim)
‚îÇ   ‚îú‚îÄ‚îÄ faiss_metadata.json         # Product metadata
‚îÇ   ‚îú‚îÄ‚îÄ reviews_faiss_index.bin     # Review chunk embeddings
‚îÇ   ‚îú‚îÄ‚îÄ reviews_metadata.json       # Review chunk metadata
‚îÇ   ‚îú‚îÄ‚îÄ evaluation_results.csv      # Phase 7 evaluation metrics
‚îÇ   ‚îî‚îÄ‚îÄ evaluation_plot.png         # MRR / NDCG comparison chart
‚îÇ
‚îú‚îÄ‚îÄ app.py                          # FastAPI backend (5 endpoints)
‚îú‚îÄ‚îÄ gradio_app.py                   # Gradio frontend (3-tab UI)
‚îú‚îÄ‚îÄ agent.py                        # LangGraph agent module
‚îú‚îÄ‚îÄ session.py                      # Session memory module
‚îú‚îÄ‚îÄ requirements.txt                # Dependencies
‚îÇ
‚îú‚îÄ‚îÄ data_preparation.ipynb          # Phase 1 ‚Äî data cleaning
‚îú‚îÄ‚îÄ semantic_retrieval.ipynb        # Phase 2 ‚Äî FAISS search
‚îú‚îÄ‚îÄ reranking.ipynb                 # Phase 3 ‚Äî cross-encoder (Colab)
‚îú‚îÄ‚îÄ rag_review_summarization.ipynb  # Phase 4 ‚Äî RAG pipeline
‚îú‚îÄ‚îÄ langgraph_agent.ipynb           # Phase 5 ‚Äî LangGraph agent
‚îú‚îÄ‚îÄ session_personalization.ipynb   # Phase 6 ‚Äî session memory
‚îú‚îÄ‚îÄ evaluation.ipynb                # Phase 7 ‚Äî NDCG + MRR
‚îú‚îÄ‚îÄ finetuning.ipynb                # Phase 8 ‚Äî LoRA fine-tuning (Colab)
‚îî‚îÄ‚îÄ deployment.ipynb                # Phase 9 ‚Äî HF Spaces deployment
```

---

## üîß Tech Stack

| Component | Technology |
|---|---|
| Embeddings | `sentence-transformers/all-MiniLM-L6-v2` |
| Vector Search | FAISS (cosine similarity) |
| Reranking | `cross-encoder/ms-marco-MiniLM-L-6-v2` |
| LLM | Groq API ‚Äî `llama-3.1-8b-instant` |
| Agent Framework | LangGraph |
| Fine-tuning | LoRA via PEFT ‚Äî `google/flan-t5-base` |
| Backend | FastAPI + Uvicorn |
| Frontend | Gradio 5 |
| Hosting | Hugging Face Spaces |
| Dataset | Amazon Reviews 2023 ‚Äî Home & Kitchen |

---

## üóÇÔ∏è Dataset

- **Source:** [Amazon Reviews 2023](https://amazon-reviews-2023.github.io/) ‚Äî Home & Kitchen category
- **Products:** 10,000
- **Reviews:** 5,566 chunks across 2,003 products (20% coverage)
- **Fields used:** title, description, price, rating, rating count, ASIN

> **Note:** 80% of products have no review text in the dataset. This is a data collection limitation ‚Äî the RAG pipeline works correctly for all products that have reviews, and gracefully returns "No reviews available" for others.

---

## üöÄ Running Locally

### Prerequisites
- Python 3.10+
- Groq API key (free at [console.groq.com](https://console.groq.com))

### Setup

```bash
# Clone the repo
git clone https://github.com/RithikaBaskaran/ShopSense.git
cd ShopSense

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Add your Groq API key
echo "GROQ_API_KEY=your_key_here" > .env
```

### Run the Gradio UI

```bash
python gradio_app.py
# Open http://localhost:7860
```

### Run the FastAPI backend

```bash
uvicorn app:app --reload
# API docs at http://localhost:8000/docs
```

### API Endpoints

| Method | Endpoint | Description |
|---|---|---|
| `GET` | `/` | Health check |
| `POST` | `/search` | Main search endpoint |
| `POST` | `/feedback` | Record like / dismiss |
| `GET` | `/session` | Get session summary |
| `POST` | `/session/reset` | Clear session memory |

---

## üìì Phase Breakdown

| Phase | Notebook | Environment | Description |
|---|---|---|---|
| 1 | `data_preparation.ipynb` | VS Code | Load, clean and sample Amazon dataset |
| 2 | `semantic_retrieval.ipynb` | VS Code | FAISS index + semantic search |
| 3 | `reranking.ipynb` | Colab (GPU) | Cross-encoder reranking |
| 4 | `rag_review_summarization.ipynb` | VS Code | Review RAG pipeline |
| 5 | `langgraph_agent.ipynb` | VS Code | LangGraph agent with intent extraction |
| 6 | `session_personalization.ipynb` | VS Code | Session memory + personalization scoring |
| 7 | `evaluation.ipynb` | VS Code | NDCG + MRR evaluation |
| 8 | `finetuning.ipynb` | Colab (GPU) | LoRA fine-tuning of flan-t5-base |
| 9 | `deployment.ipynb` | VS Code | FastAPI + Gradio + HF Spaces |

---

## üîç Example Queries

Try these on the live demo:

```
non stick frying pan under $30
compact coffee maker for small kitchen
highly rated storage bins for closet
gift for someone who loves cooking under $25
best knife set under $50 with 4 stars and above
durable water bottle for gym
cute kitchen decor for modern home
```

---

## üß† Session Personalization

ShopSense gets smarter the longer you use it within a session.

**Explicit signals** (you tell it directly):
- üëç Like a product ‚Üí future results biased toward similar items
- üëé Dismiss a product ‚Üí future results penalize that item

**Implicit signals** (extracted automatically):
- Keywords from your searches ‚Üí influence query refinement
- Price range of liked items ‚Üí biases future price scoring
- Products already shown ‚Üí slight novelty penalty to surface new items

The **Session Memory tab** shows exactly what ShopSense has learned about you at any point.

---

## ‚öôÔ∏è Fine-tuning Notes

Phase 8 fine-tuned `google/flan-t5-base` using LoRA on 118 synthetically generated product explanation examples.

- **Method:** LoRA (`r=8`, `lora_alpha=16`, 0.356% trainable params)
- **Data:** 118 examples generated via Groq Llama 3
- **Training:** 20 epochs, loss reduced from 17.4 ‚Üí 7.9
- **Key fix:** Tied weights initialization bug in flan-t5 checkpoint loader causing `nan` loss ‚Äî resolved by explicitly setting `tie_word_embeddings=True` and manually assigning `lm_head.weight = shared.weight`
- **Limitation:** 106 training examples is below the ~1,000 minimum for meaningful text quality improvement ‚Äî production deployment uses Groq Llama 3 for explanation generation

---

## üìÑ License

MIT License ‚Äî see [LICENSE](LICENSE) for details.

---

## üôè Acknowledgements

- [Amazon Reviews 2023](https://amazon-reviews-2023.github.io/) dataset
- [Groq](https://groq.com) for fast LLM inference
- [Hugging Face](https://huggingface.co) for model hosting and Spaces
- [LangGraph](https://langchain-ai.github.io/langgraph/) for agent framework
